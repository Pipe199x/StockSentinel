name: ETL Stocks (cron + manual)

on:
  schedule:
    - cron: "*/5 * * * *"   # verificaciÃ³n: cada 5 min (UTC)
  workflow_dispatch: {}

jobs:
  etl:
    runs-on: ubuntu-latest
    env:
      CONTAINER: datasets
      DEST_ROOT: stocks

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r stocks_etl/requirements.txt
          pip install pyyaml

      - name: Run ETL (un solo dataset + predicciones + heartbeat)
        run: |
          python -m stocks_etl.stocks_etl --config stocks_etl/config.yaml

      - name: Upload CSVs + heartbeat to Azure Blob (datasets/stocks/)
        uses: azure/cli@v2
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
        with:
          inlineScript: |
            # Crear contenedor si no existe
            az storage container create \
              --name "$CONTAINER" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" 1>/dev/null

            # Subir CSVs (prices_*.csv, predictions_*.csv)
            az storage blob upload-batch \
              --destination "$CONTAINER" \
              --source data \
              --pattern "*.csv*" \
              --destination-path "$DEST_ROOT" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
              --overwrite

            # Subir heartbeat (marca de vida)
            az storage blob upload \
              --container-name "$CONTAINER" \
              --file data/heartbeat.txt \
              --name "$DEST_ROOT/heartbeat.txt" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
              --overwrite
