name: ETL News (cron + manual)

on:
  schedule:
    - cron: "15 12 * * *"   # ajústalo a tu gusto
  workflow_dispatch: {}

jobs:
  etl_news:
    runs-on: ubuntu-latest

    env:
      CONTAINER: datasets
      DEST_ROOT: news
      PYTHON_VERSION: "3.11"
      TICKERS: "AMZN,MSFT,GOOGL"
      LANGUAGE: "en"
      DAYS: "15"                 # si tu news_client soporta --days

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r news_etl/requirements.txt
          pip install pyyaml

      - name: Prepare data dir
        run: mkdir -p data

      # ---------------- Genera RAW con tu news_client ----------------
      - name: Generate raw with NewsAPI (your news_client)
        env:
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
        run: |
          set -euxo pipefail
          # Si tu news_client usa NEWSAPI_KEY desde os.environ, ya está exportado por env:
          python -m news_etl.news_client \
            --out data/news_raw.ndjson \
            --tickers "${TICKERS}" \
            --language "${LANGUAGE}" \
            --days "${DAYS}"
          echo "=== data tras news_client ==="
          ls -l data || true
          # (opcional) valida que el RAW exista y tenga contenido; si no quieres que falle, comenta la línea siguiente:
          test -s data/news_raw.ndjson

      # ---------------- Ejecuta ETL con tu config ----------------
      - name: Run ETL
        env:
          AZURE_LANGUAGE_KEY: ${{ secrets.AZURE_LANGUAGE_KEY }}
          AZURE_LANGUAGE_ENDPOINT: ${{ secrets.AZURE_LANGUAGE_ENDPOINT }}
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          SAMPLE_LIMIT: "0"
        run: |
          set -euxo pipefail
          python -m news_etl.news_etl --config news_etl/config.yaml
          echo "=== data tras ETL ==="
          ls -l data || true

      # ---------------- Sube CSVs + heartbeat a Blob ----------------
      - name: Upload CSVs + heartbeat to Azure Blob (datasets/news/)
        uses: azure/cli@v2
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
        with:
          inlineScript: |
            set -euxo pipefail
            az storage container create \
              --name "$CONTAINER" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" 1>/dev/null
            az storage blob upload-batch \
              --destination "$CONTAINER" \
              --source data \
              --pattern "*.csv*" \
              --destination-path "$DEST_ROOT" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
              --overwrite
            az storage blob upload \
              --container-name "$CONTAINER" \
              --file data/heartbeat.txt \
              --name "$DEST_ROOT/heartbeat.txt" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
              --overwrite

      # ---------------- Limpieza RAW (opcional) ----------------
      - name: Cleanup local raw
        if: always()
        run: rm -f data/news_raw.ndjson || true
