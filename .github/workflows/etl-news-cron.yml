name: ETL News (cron + manual)

on:
  schedule:
    - cron: "15 12 * * *"     # diario 12:15 UTC (ajÃºstalo si quieres)
  workflow_dispatch: {}

jobs:
  etl_news:
    runs-on: ubuntu-latest

    env:
      CONTAINER: datasets      # tu contenedor en Azure Blob
      DEST_ROOT: news          # subcarpeta dentro del contenedor (datasets/news)
      PYTHON_VERSION: "3.11"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r news_etl/requirements.txt
          pip install pyyaml

      - name: Prepare data dir
        run: mkdir -p data

      - name: Generate raw with NewsAPI
        env:
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
        run: |
          set -e
          python -m news_etl.news_client \
            --out data/news_raw.ndjson \
            --tickers "AMZN,MSFT,GOOGL" \
            --language en
          # Asegura que el raw exista y tenga contenido
          test -s data/news_raw.ndjson

      - name: Run ETL
        env:
          AZURE_LANGUAGE_KEY: ${{ secrets.AZURE_LANGUAGE_KEY }}
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
          SAMPLE_LIMIT: "0"
        run: |
          set -e
          python -m news_etl.news_etl --config news_etl/config.yaml
          ls -l data

      - name: Upload CSVs + heartbeat to Azure Blob (datasets/news/)
        uses: azure/cli@v2
        env:
          AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
        with:
          inlineScript: |
            set -e
            az storage container create \
              --name "$CONTAINER" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" 1>/dev/null
            az storage blob upload-batch \
              --destination "$CONTAINER" \
              --source data \
              --pattern "*.csv*" \
              --destination-path "$DEST_ROOT" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
              --overwrite
            az storage blob upload \
              --container-name "$CONTAINER" \
              --file data/heartbeat.txt \
              --name "$DEST_ROOT/heartbeat.txt" \
              --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
              --overwrite

      - name: Cleanup local raw
        if: always()
        run: rm -f data/news_raw.ndjson || true
