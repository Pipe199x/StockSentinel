name: etl

on:
  workflow_dispatch:
  schedule:
    - cron: "0 15 * * 1-5" # L-V 15:00 UTC (ajústalo si quieres)

jobs:
  etl:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    env:
      # El ETL usa estos secrets
      AZURE_LANGUAGE_ENDPOINT: ${{ secrets.AZURE_LANGUAGE_ENDPOINT }}
      AZURE_LANGUAGE_KEY: ${{ secrets.AZURE_LANGUAGE_KEY }}
      AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
      # Si quieres pruebas con menos items, pon "SAMPLE_LIMIT" en 50 o similar; "0" = sin límite
      SAMPLE_LIMIT: "0"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r news_etl/requirements.txt

      # 1) Crea carpeta de trabajo local (temporal del runner)
      - name: Prepare data dir
        run: mkdir -p data

      # 2) Genera el RAW local (temporal) que consume el ETL
      - name: Generate raw with NewsAPI
        env:
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
        run: |
          python -m news_etl.news_client \
            --out data/news_raw.ndjson \
            --tickers AMZN,MSFT,GOOGL \
            --language en
          echo "=== Listando data/ ==="
          ls -l data

      # 3) Ejecuta el ETL: leerá el RAW, procesará con Azure Language
      #    y subirá SOLO los CSV a tu Blob bajo datasets/news/
      - name: Run ETL
        run: |
          # Verificación defensiva
          test -f data/news_raw.ndjson || { echo "Falta data/news_raw.ndjson"; exit 1; }
          python -m news_etl.news_etl --config news_etl/config.yaml

      # 4) Limpia el RAW local para que no quede nada más que los CSV en el Blob
      - name: Cleanup local raw
        if: always()
        run: |
          rm -f data/news_raw.ndjson || true
          echo "Limpieza hecha; los CSV quedaron en Blob (datasets/news/)."
