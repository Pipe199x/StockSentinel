name: ETL News (StockSentinel)

on:
  schedule:
    - cron: "45 22 * * *"  # diario 22:45 UTC
    # - cron: "*/5 * * * *"  # <- Variante de prueba (descomenta si quieres)
  workflow_dispatch:

permissions:
  contents: read

env:
  AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
  NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
  AZURE_LANGUAGE_ENDPOINT: ${{ secrets.AZURE_LANGUAGE_ENDPOINT }}
  AZURE_LANGUAGE_KEY: ${{ secrets.AZURE_LANGUAGE_KEY }}

jobs:
  etl-news:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (news)
        run: |
          python -m pip install --upgrade pip
          pip install -r news_etl/requirements.txt

      - name: Run ETL News
        run: |
          python -m news_etl.news_etl --config news_etl/config.yaml

      - name: Azure CLI
        uses: azure/cli@v2

      - name: Upload NDJSON to Azure Blob (datasets/news/)
        if: env.AZURE_STORAGE_CONNECTION_STRING != ''
        run: |
          az storage blob upload-batch \
            --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
            --source "data" \
            --destination "datasets/news" \
            --pattern "news_*.ndjson" \
            --overwrite true

      - name: Upload preview + heartbeat
        if: env.AZURE_STORAGE_CONNECTION_STRING != ''
        run: |
          az storage blob upload-batch \
            --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
            --source "data" \
            --destination "datasets/news" \
            --pattern "news_preview_*.csv" \
            --overwrite true
          az storage blob upload \
            --connection-string "$AZURE_STORAGE_CONNECTION_STRING" \
            --file "data/heartbeat.txt" \
            --container-name "datasets" \
            --name "news/heartbeat.txt" \
            --overwrite true
